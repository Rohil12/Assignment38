{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb5c37b-d14e-4e34-8abf-ad1e9330cb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Overfitting:\\nDefinition: Overfitting occurs when a model learns not only the underlying patterns but also the noise and details of the training data. It performs exceptionally well on the training set but poorly on new, unseen data. Consequences: Poor generalization to new data, high variance, and often misleadingly high performance metrics on training data. Mitigation:\\n\\nCross-Validation: Using techniques like k-fold cross-validation to ensure the model's performance is consistent across different subsets of data.\\n\\nRegularization: Adding a penalty term to the model to prevent it from becoming too complex (e.g., Lasso, Ridge).\\n\\nPruning: For decision trees, removing branches that have little importance.\\n\\nSimpler Models: Sometimes, using a simpler model can reduce overfitting.\\n\\nUnderfitting:\\nDefinition: Underfitting occurs when a model is too simplistic and fails to capture the underlying patterns of the data. It performs poorly on both training and new data. Consequences: Inability to make accurate predictions, high bias, and consistently poor performance metrics. Mitigation:\\n\\nMore Features: Adding more relevant features to the model to help it capture patterns.\\n\\nComplexer Models: Using more sophisticated algorithms or increasing the complexity of the model.\\n\\nFeature Engineering: Transforming raw data into meaningful features that better represent the underlying problem to the model\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q.1 \n",
    "\"\"\"Overfitting:\n",
    "Definition: Overfitting occurs when a model learns not only the underlying patterns but also the noise and details of the training data. It performs exceptionally well on the training set but poorly on new, unseen data. Consequences: Poor generalization to new data, high variance, and often misleadingly high performance metrics on training data. Mitigation:\n",
    "\n",
    "Cross-Validation: Using techniques like k-fold cross-validation to ensure the model's performance is consistent across different subsets of data.\n",
    "\n",
    "Regularization: Adding a penalty term to the model to prevent it from becoming too complex (e.g., Lasso, Ridge).\n",
    "\n",
    "Pruning: For decision trees, removing branches that have little importance.\n",
    "\n",
    "Simpler Models: Sometimes, using a simpler model can reduce overfitting.\n",
    "\n",
    "Underfitting:\n",
    "Definition: Underfitting occurs when a model is too simplistic and fails to capture the underlying patterns of the data. It performs poorly on both training and new data. Consequences: Inability to make accurate predictions, high bias, and consistently poor performance metrics. Mitigation:\n",
    "\n",
    "More Features: Adding more relevant features to the model to help it capture patterns.\n",
    "\n",
    "Complexer Models: Using more sophisticated algorithms or increasing the complexity of the model.\n",
    "\n",
    "Feature Engineering: Transforming raw data into meaningful features that better represent the underlying problem to the model\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1910cdd-ebd5-4933-beb9-3ad8cbd40e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You\\nHow can we reduce overfitting? Explain in brief.\\nCopilot\\nReducing overfitting is key to ensuring your model performs well on new, unseen data. Here are some strategies:\\n\\nCross-Validation: Use techniques like k-fold cross-validation to validate your model on different subsets of the data and ensure it generalizes well.\\n\\nRegularization: Add a penalty term to the model's cost function to discourage complexity (e.g., Lasso, Ridge).\\n\\nPruning: For decision trees, remove branches that have little importance to prevent the model from becoming too detailed.\\n\\nSimpler Models: Sometimes, using a less complex model can prevent overfitting by not trying to capture every tiny detail.\\n\\nData Augmentation: Increase the amount of training data by slightly altering the existing data (e.g., rotating images, adding noise).\\n\\nEarly Stopping: During training, stop the model when performance on a validation set starts to deteriorate rather than letting it learn all the noise.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q.2\n",
    "\"\"\"You\n",
    "How can we reduce overfitting? Explain in brief.\n",
    "Copilot\n",
    "Reducing overfitting is key to ensuring your model performs well on new, unseen data. Here are some strategies:\n",
    "\n",
    "Cross-Validation: Use techniques like k-fold cross-validation to validate your model on different subsets of the data and ensure it generalizes well.\n",
    "\n",
    "Regularization: Add a penalty term to the model's cost function to discourage complexity (e.g., Lasso, Ridge).\n",
    "\n",
    "Pruning: For decision trees, remove branches that have little importance to prevent the model from becoming too detailed.\n",
    "\n",
    "Simpler Models: Sometimes, using a less complex model can prevent overfitting by not trying to capture every tiny detail.\n",
    "\n",
    "Data Augmentation: Increase the amount of training data by slightly altering the existing data (e.g., rotating images, adding noise).\n",
    "\n",
    "Early Stopping: During training, stop the model when performance on a validation set starts to deteriorate rather than letting it learn all the noise.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815fcb9c-d210-4e9b-9854-e9dec5708ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Underfitting\\nDefinition: Underfitting occurs when a machine learning model is too simplistic to capture the underlying patterns in the data. It performs poorly on both the training set and unseen data because it hasn't learned the essential trends and relationships.\\n\\nScenarios Where Underfitting Can Occur:\\nModel Simplicity: Using a linear model for a problem that requires a more complex relationship, such as fitting a straight line to data that follows a quadratic pattern.\\n\\nInsufficient Training: Not training the model for enough epochs, especially in deep learning, resulting in the model not learning the data's complexity.\\n\\nFeature Selection: Using too few or irrelevant features that don't adequately represent the underlying patterns in the data.\\n\\nHigh Regularization: Applying too much regularization, which can overly constrain the model and prevent it from fitting the training data well.\\n\\nLow Model Capacity: Choosing a model with too few parameters, such as a shallow neural network, which lacks the capacity to learn complex patterns.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q.3\n",
    "\"\"\"Underfitting\n",
    "Definition: Underfitting occurs when a machine learning model is too simplistic to capture the underlying patterns in the data. It performs poorly on both the training set and unseen data because it hasn't learned the essential trends and relationships.\n",
    "\n",
    "Scenarios Where Underfitting Can Occur:\n",
    "Model Simplicity: Using a linear model for a problem that requires a more complex relationship, such as fitting a straight line to data that follows a quadratic pattern.\n",
    "\n",
    "Insufficient Training: Not training the model for enough epochs, especially in deep learning, resulting in the model not learning the data's complexity.\n",
    "\n",
    "Feature Selection: Using too few or irrelevant features that don't adequately represent the underlying patterns in the data.\n",
    "\n",
    "High Regularization: Applying too much regularization, which can overly constrain the model and prevent it from fitting the training data well.\n",
    "\n",
    "Low Model Capacity: Choosing a model with too few parameters, such as a shallow neural network, which lacks the capacity to learn complex patterns.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75af74f8-21b8-4df1-b01e-2ad0d9fb357c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bias:\\n\\nDefinition: Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias means the model is too simple and does not capture the underlying patterns.\\n\\nImpact: High bias typically leads to underfitting, where the model performs poorly on both training and new data.\\n\\nVariance:\\n\\nDefinition: Variance refers to the error introduced by the modelâ€™s sensitivity to small fluctuations in the training data. High variance means the model is too complex and fits the noise in the training data.\\n\\nImpact: High variance typically leads to overfitting, where the model performs well on training data but poorly on new data.\\n\\nTradeoff:\\nThe bias-variance tradeoff represents the balance between the errors due to bias and variance. Ideally, we want to minimize both to create a model that generalizes well to new data.\\n\\nRelationship:\\nHigh Bias + Low Variance: Simple models (e.g., linear regression) often have high bias and low variance. They make strong assumptions about the data, which can result in missing the underlying patterns.\\n\\nLow Bias + High Variance: Complex models (e.g., deep learning) often have low bias and high variance. They can capture more nuances in the training data but may also fit the noise, leading to overfitting.\\n\\nEffects on Model Performance:\\nHigh Bias: Leads to underfitting. The model is too simplistic and performs poorly on both training and test data.\\n\\nHigh Variance: Leads to overfitting. The model is too complex and performs well on training data but poorly on test data.\\n\\nOptimal Tradeoff: Finding the sweet spot where both bias and variance are minimized results in the best model performance. This is achieved by techniques like cross-validation, regularization, and pruning.\\n\\nVisual Representation:\\nImagine a bullseye target:\\n\\nHigh Bias, Low Variance: Consistently misses the bullseye but all shots are close together.\\n\\nLow Bias, High Variance: Hits all over the target, including the bullseye, but not consistently.\\n\\nOptimal Tradeoff: Consistently hits close to the bullseye with little spread.\\n\\nBalancing bias and variance is crucial for building robust models that perform well on new, unseen data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q.4\n",
    "\"\"\"Bias:\n",
    "\n",
    "Definition: Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias means the model is too simple and does not capture the underlying patterns.\n",
    "\n",
    "Impact: High bias typically leads to underfitting, where the model performs poorly on both training and new data.\n",
    "\n",
    "Variance:\n",
    "\n",
    "Definition: Variance refers to the error introduced by the modelâ€™s sensitivity to small fluctuations in the training data. High variance means the model is too complex and fits the noise in the training data.\n",
    "\n",
    "Impact: High variance typically leads to overfitting, where the model performs well on training data but poorly on new data.\n",
    "\n",
    "Tradeoff:\n",
    "The bias-variance tradeoff represents the balance between the errors due to bias and variance. Ideally, we want to minimize both to create a model that generalizes well to new data.\n",
    "\n",
    "Relationship:\n",
    "High Bias + Low Variance: Simple models (e.g., linear regression) often have high bias and low variance. They make strong assumptions about the data, which can result in missing the underlying patterns.\n",
    "\n",
    "Low Bias + High Variance: Complex models (e.g., deep learning) often have low bias and high variance. They can capture more nuances in the training data but may also fit the noise, leading to overfitting.\n",
    "\n",
    "Effects on Model Performance:\n",
    "High Bias: Leads to underfitting. The model is too simplistic and performs poorly on both training and test data.\n",
    "\n",
    "High Variance: Leads to overfitting. The model is too complex and performs well on training data but poorly on test data.\n",
    "\n",
    "Optimal Tradeoff: Finding the sweet spot where both bias and variance are minimized results in the best model performance. This is achieved by techniques like cross-validation, regularization, and pruning.\n",
    "\n",
    "Visual Representation:\n",
    "Imagine a bullseye target:\n",
    "\n",
    "High Bias, Low Variance: Consistently misses the bullseye but all shots are close together.\n",
    "\n",
    "Low Bias, High Variance: Hits all over the target, including the bullseye, but not consistently.\n",
    "\n",
    "Optimal Tradeoff: Consistently hits close to the bullseye with little spread.\n",
    "\n",
    "Balancing bias and variance is crucial for building robust models that perform well on new, unseen data\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5848476-6ae7-4efe-a598-428cb67b995d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Detecting Overfitting and Underfitting\\n1. Cross-Validation:\\n\\nMethod: Use techniques like k-fold cross-validation to evaluate the model on different subsets of the data.\\n\\nDetection: If the model performs well on training data but poorly on validation data, it indicates overfitting. Consistently poor performance on both training and validation data suggests underfitting.\\n\\n2. Learning Curves:\\n\\nMethod: Plot the modelâ€™s training and validation error as functions of the training set size.\\n\\nDetection:\\n\\nOverfitting: Large gap between training and validation errors, with low training error.\\n\\nUnderfitting: Both training and validation errors are high and close to each other.\\n\\n3. Performance Metrics:\\n\\nMethod: Compare metrics like accuracy, precision, recall, and F1 score on training and validation sets.\\n\\nDetection:\\n\\nOverfitting: High metrics on training data but significantly lower metrics on validation data.\\n\\nUnderfitting: Low metrics on both training and validation data.\\n\\n4. Validation Set:\\n\\nMethod: Set aside a validation set during training to monitor the modelâ€™s performance.\\n\\nDetection:\\n\\nOverfitting: Performance on validation set is much worse than on training set.\\n\\nUnderfitting: Poor performance on both training and validation sets.\\n\\n5. Model Complexity:\\n\\nMethod: Experiment with different model complexities (e.g., varying the number of layers in neural networks).\\n\\nDetection:\\n\\nOverfitting: Complex models with many parameters perform well on training data but poorly on validation data.\\n\\nUnderfitting: Simple models with few parameters perform poorly on both training and validation data.\\n\\nDetermining Overfitting or Underfitting\\nOverfitting:\\n\\nSymptoms: Excellent performance on training data but poor performance on validation/test data.\\n\\nSolutions: Cross-validation, regularization, pruning, simplifying the model, adding more training data, early stopping.\\n\\nUnderfitting:\\n\\nSymptoms: Poor performance on both training and validation/test data.\\n\\nSolutions: More relevant features, increasing model complexity, reducing regularization, feature engineering.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q.5\n",
    "\"\"\"Detecting Overfitting and Underfitting\n",
    "1. Cross-Validation:\n",
    "\n",
    "Method: Use techniques like k-fold cross-validation to evaluate the model on different subsets of the data.\n",
    "\n",
    "Detection: If the model performs well on training data but poorly on validation data, it indicates overfitting. Consistently poor performance on both training and validation data suggests underfitting.\n",
    "\n",
    "2. Learning Curves:\n",
    "\n",
    "Method: Plot the modelâ€™s training and validation error as functions of the training set size.\n",
    "\n",
    "Detection:\n",
    "\n",
    "Overfitting: Large gap between training and validation errors, with low training error.\n",
    "\n",
    "Underfitting: Both training and validation errors are high and close to each other.\n",
    "\n",
    "3. Performance Metrics:\n",
    "\n",
    "Method: Compare metrics like accuracy, precision, recall, and F1 score on training and validation sets.\n",
    "\n",
    "Detection:\n",
    "\n",
    "Overfitting: High metrics on training data but significantly lower metrics on validation data.\n",
    "\n",
    "Underfitting: Low metrics on both training and validation data.\n",
    "\n",
    "4. Validation Set:\n",
    "\n",
    "Method: Set aside a validation set during training to monitor the modelâ€™s performance.\n",
    "\n",
    "Detection:\n",
    "\n",
    "Overfitting: Performance on validation set is much worse than on training set.\n",
    "\n",
    "Underfitting: Poor performance on both training and validation sets.\n",
    "\n",
    "5. Model Complexity:\n",
    "\n",
    "Method: Experiment with different model complexities (e.g., varying the number of layers in neural networks).\n",
    "\n",
    "Detection:\n",
    "\n",
    "Overfitting: Complex models with many parameters perform well on training data but poorly on validation data.\n",
    "\n",
    "Underfitting: Simple models with few parameters perform poorly on both training and validation data.\n",
    "\n",
    "Determining Overfitting or Underfitting\n",
    "Overfitting:\n",
    "\n",
    "Symptoms: Excellent performance on training data but poor performance on validation/test data.\n",
    "\n",
    "Solutions: Cross-validation, regularization, pruning, simplifying the model, adding more training data, early stopping.\n",
    "\n",
    "Underfitting:\n",
    "\n",
    "Symptoms: Poor performance on both training and validation/test data.\n",
    "\n",
    "Solutions: More relevant features, increasing model complexity, reducing regularization, feature engineering.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bc8f05f-edca-4f95-ad7f-7d72909a8e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bias and Variance in Machine Learning\\nBias:\\n\\nDefinition: Bias refers to the error introduced by assuming a simplified model to represent complex real-world relationships. High bias indicates the model is too simple and underfits the data.\\n\\nPerformance: High bias models generally perform poorly on both training and test data.\\n\\nExample: Linear regression applied to non-linear data.\\n\\nVariance:\\n\\nDefinition: Variance refers to the error introduced when a model is too sensitive to small fluctuations in the training data. High variance indicates the model is too complex and overfits the data.\\n\\nPerformance: High variance models perform well on training data but poorly on test data due to overfitting.\\n\\nExample: A deep neural network with many layers and parameters trained on a small dataset.\\n\\nComparison:\\nHigh Bias:\\n\\nCharacteristics: Simpler models, make strong assumptions, underfit the data.\\n\\nPerformance: Consistent errors, poor prediction on both training and test sets.\\n\\nExample: Linear regression on a complex, non-linear dataset.\\n\\nHigh Variance:\\n\\nCharacteristics: Complex models, highly sensitive to training data, overfit the data.\\n\\nPerformance: Excellent prediction on training set but poor prediction on test set.\\n\\nExample: A deep neural network trained with few training samples.\\n\\nVisual Representation:\\nHigh Bias: Think of trying to fit a straight line to data that clearly forms a curve; the line misses the pattern entirely.\\n\\nHigh Variance: Imagine fitting a highly intricate curve that touches all the points perfectly in your training data, but goes wildly off course with new data points.\\n\\nFinding Balance:\\nThe goal is to strike a balance between bias and variance to build a model that generalizes well to new data. This is achieved through techniques like cross-validation, regularization, and choosing the right complexity of the model'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q.6\n",
    "\"\"\"Bias and Variance in Machine Learning\n",
    "Bias:\n",
    "\n",
    "Definition: Bias refers to the error introduced by assuming a simplified model to represent complex real-world relationships. High bias indicates the model is too simple and underfits the data.\n",
    "\n",
    "Performance: High bias models generally perform poorly on both training and test data.\n",
    "\n",
    "Example: Linear regression applied to non-linear data.\n",
    "\n",
    "Variance:\n",
    "\n",
    "Definition: Variance refers to the error introduced when a model is too sensitive to small fluctuations in the training data. High variance indicates the model is too complex and overfits the data.\n",
    "\n",
    "Performance: High variance models perform well on training data but poorly on test data due to overfitting.\n",
    "\n",
    "Example: A deep neural network with many layers and parameters trained on a small dataset.\n",
    "\n",
    "Comparison:\n",
    "High Bias:\n",
    "\n",
    "Characteristics: Simpler models, make strong assumptions, underfit the data.\n",
    "\n",
    "Performance: Consistent errors, poor prediction on both training and test sets.\n",
    "\n",
    "Example: Linear regression on a complex, non-linear dataset.\n",
    "\n",
    "High Variance:\n",
    "\n",
    "Characteristics: Complex models, highly sensitive to training data, overfit the data.\n",
    "\n",
    "Performance: Excellent prediction on training set but poor prediction on test set.\n",
    "\n",
    "Example: A deep neural network trained with few training samples.\n",
    "\n",
    "Visual Representation:\n",
    "High Bias: Think of trying to fit a straight line to data that clearly forms a curve; the line misses the pattern entirely.\n",
    "\n",
    "High Variance: Imagine fitting a highly intricate curve that touches all the points perfectly in your training data, but goes wildly off course with new data points.\n",
    "\n",
    "Finding Balance:\n",
    "The goal is to strike a balance between bias and variance to build a model that generalizes well to new data. This is achieved through techniques like cross-validation, regularization, and choosing the right complexity of the model\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a4ff562-0644-4f17-9425-76473c807e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Regularization in Machine Learning\\nDefinition: Regularization is a technique used to prevent overfitting by adding a penalty term to the model's cost function. This penalty term discourages the model from becoming overly complex by ensuring that the learned parameters (weights) remain small.\\n\\nHow Regularization Prevents Overfitting:\\nOverfitting happens when a model learns the noise in the training data instead of the underlying patterns. Regularization controls this by adding a constraint to the learning process, which prevents the model from fitting the noise.\\n\\nCommon Regularization Techniques:\\nL1 Regularization (Lasso):\\n\\nHow it Works: Adds the absolute value of the magnitude of the coefficients as a penalty term to the cost function.\\n\\nCost Function: \\\\[ \\text{Cost} = \\text{Loss} + \\\\lambda \\\\sum |w_i| \\\\]\\n\\nEffect: Encourages sparsity, meaning it can shrink some coefficients to zero, effectively performing feature selection.\\n\\nL2 Regularization (Ridge):\\n\\nHow it Works: Adds the squared magnitude of the coefficients as a penalty term to the cost function.\\n\\nCost Function: \\\\[ \\text{Cost} = \\text{Loss} + \\\\lambda \\\\sum w_i^2 \\\\]\\n\\nEffect: Penalizes large coefficients more strongly, reducing the complexity of the model.\\n\\nElastic Net Regularization:\\n\\nHow it Works: Combines both L1 and L2 regularization terms.\\n\\nCost Function: \\\\[ \\text{Cost} = \\text{Loss} + \\\\lambda_1 \\\\sum |w_i| + \\\\lambda_2 \\\\sum w_i^2 \\\\]\\n\\nEffect: Balances the benefits of both Lasso and Ridge regularization.\\n\\nDropout (specific to neural networks):\\n\\nHow it Works: Randomly drops units (neurons) during training with a certain probability.\\n\\nEffect: Prevents the network from becoming too reliant on particular neurons and forces it to generalize better by introducing redundancy.\\n\\nEarly Stopping:\\n\\nHow it Works: Monitors the modelâ€™s performance on a validation set and stops training when performance starts to deteriorate.\\n\\nEffect: Prevents the model from overfitting by not letting it train too long on the training data.\\n\\nExample:\\nImagine training a linear regression model with many features. Without regularization, the model may assign very high or low weights to features to minimize the training error, potentially overfitting the training data. With L2 regularization, the penalty term forces the model to keep the weights small, promoting a simpler model that generalizes better to new data.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q.7\n",
    "\"\"\"Regularization in Machine Learning\n",
    "Definition: Regularization is a technique used to prevent overfitting by adding a penalty term to the model's cost function. This penalty term discourages the model from becoming overly complex by ensuring that the learned parameters (weights) remain small.\n",
    "\n",
    "How Regularization Prevents Overfitting:\n",
    "Overfitting happens when a model learns the noise in the training data instead of the underlying patterns. Regularization controls this by adding a constraint to the learning process, which prevents the model from fitting the noise.\n",
    "\n",
    "Common Regularization Techniques:\n",
    "L1 Regularization (Lasso):\n",
    "\n",
    "How it Works: Adds the absolute value of the magnitude of the coefficients as a penalty term to the cost function.\n",
    "\n",
    "Cost Function: \\[ \\text{Cost} = \\text{Loss} + \\lambda \\sum |w_i| \\]\n",
    "\n",
    "Effect: Encourages sparsity, meaning it can shrink some coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "L2 Regularization (Ridge):\n",
    "\n",
    "How it Works: Adds the squared magnitude of the coefficients as a penalty term to the cost function.\n",
    "\n",
    "Cost Function: \\[ \\text{Cost} = \\text{Loss} + \\lambda \\sum w_i^2 \\]\n",
    "\n",
    "Effect: Penalizes large coefficients more strongly, reducing the complexity of the model.\n",
    "\n",
    "Elastic Net Regularization:\n",
    "\n",
    "How it Works: Combines both L1 and L2 regularization terms.\n",
    "\n",
    "Cost Function: \\[ \\text{Cost} = \\text{Loss} + \\lambda_1 \\sum |w_i| + \\lambda_2 \\sum w_i^2 \\]\n",
    "\n",
    "Effect: Balances the benefits of both Lasso and Ridge regularization.\n",
    "\n",
    "Dropout (specific to neural networks):\n",
    "\n",
    "How it Works: Randomly drops units (neurons) during training with a certain probability.\n",
    "\n",
    "Effect: Prevents the network from becoming too reliant on particular neurons and forces it to generalize better by introducing redundancy.\n",
    "\n",
    "Early Stopping:\n",
    "\n",
    "How it Works: Monitors the modelâ€™s performance on a validation set and stops training when performance starts to deteriorate.\n",
    "\n",
    "Effect: Prevents the model from overfitting by not letting it train too long on the training data.\n",
    "\n",
    "Example:\n",
    "Imagine training a linear regression model with many features. Without regularization, the model may assign very high or low weights to features to minimize the training error, potentially overfitting the training data. With L2 regularization, the penalty term forces the model to keep the weights small, promoting a simpler model that generalizes better to new data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63742d-1355-40f9-aada-c17182f0a15c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
